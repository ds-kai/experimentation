{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d1667",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy\n",
    "\n",
    "from scipy import stats\n",
    "from statsmodels.stats.descriptivestats import sign_test\n",
    "\n",
    "import arrow # date format\n",
    "import warnings; warnings.simplefilter('ignore') # do not disturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace656b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size_for_two_means(\n",
    "    effect_size: float,\n",
    "    sd: float,\n",
    "    alpha: float = 0.05,\n",
    "    power: float = 0.8,\n",
    "    df: int = 1000,\n",
    ") -> int:\n",
    "    '''Given effect size, standard deviation, alpha (type I error), power (1 - type II error)\n",
    "    and t-test df (degree of freedom), calculate minimum sample sizes required (in total),\n",
    "    assuming equal-size cells, shared standard deviation and one-sided t-test.\n",
    "    '''\n",
    "    z1 = stats.t.ppf(q=1-alpha, df=df)\n",
    "    z2 = stats.t.ppf(q=power, df=df)\n",
    "    n = 2 * sd ** 2 * ((z1 + z2) / effect_size) ** 2\n",
    "    \n",
    "    return int(np.ceil(n) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a814c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "power = 0.8\n",
    "\n",
    "sample_size_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "#mean, std = 238809 , 58714\n",
    "mean, std = df['roas'].mean(), df['roas'].std()\n",
    "\n",
    "for i in range(0,len(desired_lift)):\n",
    "    lift = desired_lift[i]\n",
    "    # lift is applied to the original scale\n",
    "    effect_size = (mean * lift)\n",
    "    sample_size = calculate_sample_size_for_two_means(\n",
    "        effect_size=effect_size,\n",
    "        sd=std,\n",
    "        alpha=alpha,\n",
    "        power=power)\n",
    "    sample_size_df.loc[i, \"Lift\"] = str(int(lift * 100)) + '%'\n",
    "    sample_size_df.loc[i, \"Effect Size\"] = effect_size\n",
    "    sample_size_df.loc[i, \"Days\"] = sample_size\n",
    "    sample_size_df.loc[i, \"Weeks\"] = int(sample_size / 7)\n",
    "    \n",
    "sample_size_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adb2b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb95d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df.select_dtypes(include=['float64', 'int64']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7455685",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(scaled_features)\n",
    "df['cluster'] = kmeans.labels_\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ab958",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "umap_embedding = umap_reducer.fit_transform(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f611ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sunday(date):\n",
    "    monday = date - timedelta(days=date.weekday())\n",
    "    # Subtract one day to get the Sunday\n",
    "    sunday = monday - timedelta(days=1)\n",
    "    return sunday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb6b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "import pmdarima as pm\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c1e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=sm.tsa.statespace.SARIMAX(\n",
    "                                train_df['new_customer_count'], \n",
    "                                exog=train_df[\"spend_eur\"],\n",
    "                                order=(2, 1, 2),\n",
    "                                seasonal_order=(2, 1, 2, 13)\n",
    "                               )\n",
    "results=model.fit()\n",
    "\n",
    "test_df['forecast']= results.predict(\n",
    "                                start=\"2023-12-24\",\n",
    "                                end=\"2024-03-10\",\n",
    "                                exog=test_df['spend_eur'],\n",
    "                                dynamic=True\n",
    ")\n",
    "\n",
    "full_forecast = pd.concat([train_df, test_df])\n",
    "full_forecast[['new_customer_count','forecast']].plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PointEstimate(df1, df2, column_name):\n",
    "    ## point estimate of the difference between two samples\n",
    "    ## input should be two vectors\n",
    "    df = df2[column_name] - df1[column_name]\n",
    "    point_estimate = df.mean()\n",
    "    std = df.std()\n",
    "    return point_estimate, std\n",
    "\n",
    "def StatSig(df1, df2, column_name): \n",
    "    pvalue = stats.ttest_ind(df1[column_name], df2[column_name]).pvalue\n",
    "    alpha = 0.05\n",
    "    if pvalue < alpha: \n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "def ConfidenceInterval(df1, df2, column_name):\n",
    "    # using 90% confidence interval \n",
    "    # point_estimate and lift are the same value\n",
    "    point_estimate, std = PointEstimate(df1, df2, column_name)\n",
    "    lift, lift_pctg = Lift(df1, df2, column_name)\n",
    "    \n",
    "    sample_size = len(df1.Day.unique())\n",
    "    confidence = 0.90\n",
    "    \n",
    "    margin_of_error = std * stats.t.ppf((1 + confidence) / 2., sample_size -1 )\n",
    "#     stats.t.interval(0.95, sample_size - 1, loc = point_estimate, scale = std)\n",
    "    lower_bound = point_estimate - margin_of_error\n",
    "    upper_bound = point_estimate + margin_of_error\n",
    "    \n",
    "    margin_of_error_pctg = margin_of_error / df1[column_name].mean()\n",
    "    \n",
    "    lower_bound_pctg = lift_pctg - margin_of_error_pctg\n",
    "    upper_bound_pctg = lift_pctg + margin_of_error_pctg\n",
    "    return (lower_bound, upper_bound), (lower_bound_pctg, upper_bound_pctg)\n",
    "\n",
    "def Lift(df1, df2, column_name):\n",
    "    # calculation the absolute and relative lift between the two arms\n",
    "    mean1 = df1[column_name].mean()\n",
    "    mean2 = df2[column_name].mean()\n",
    "    lift = mean2 - mean1\n",
    "#     lift_pctg = f\"{lift / mean1:.0%}\"\n",
    "    lift_pctg = lift / mean1\n",
    "    return lift, lift_pctg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bayes_ab.experiments import PoissonDataTest\n",
    "from bayes_ab.experiments import NormalDataTest\n",
    "from bayes_ab.experiments import BinaryDataTest\n",
    "\n",
    "# from bayesian_testing.experiments import BinaryDataTest\n",
    "# from bayesian_testing.experiments import PoissonDataTest\n",
    "# from bayesian_testing.experiments import NormalDataTest\n",
    "\n",
    "# https://pypi.org/project/bayes-ab/\n",
    "# https://pypi.org/project/bayes-ab/\n",
    "\n",
    "# https://www.pymc.io/projects/examples/en/latest/causal_inference/bayesian_ab_testing_introduction.html\n",
    "# https://support.dynamicyield.com/hc/en-us/articles/360035983014-Understanding-A-B-Test-Results#:~:text=Uplift%20is%20the%20percent%20difference,%2C%20the%20uplift%20is%2025%25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b386bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_test = PoissonDataTest()\n",
    "\n",
    "bayesian_test.add_variant_data(\"control-installations\", df_control.Installations)\n",
    "bayesian_test.add_variant_data(\"test-installations\", df_test.Installations)\n",
    "\n",
    "# evaluate test\n",
    "results = bayesian_test.evaluate()\n",
    "# results\n",
    "# print(pd.DataFrame(results).to_markdown(tablefmt=\"grid\"))\n",
    "# print(pd.DataFrame(results).set_index('variant').T.to_markdown(tablefmt=\"grid\"))\n",
    "\n",
    "# data = bayesian_test.data\n",
    "\n",
    "# generate plots\n",
    "# bayesian_test.plot_distributions(control='control-installations', test='test-installations', fname='binary_distributions_example.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa5155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a0a7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ff0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating some random data\n",
    "rng = np.random.default_rng(52)\n",
    "# random 1x1500 array of 0/1 data with 5.2% probability for 1:\n",
    "data_a = rng.binomial(n=1, p=0.052, size=1500)\n",
    "# random 1x1200 array of 0/1 data with 6.7% probability for 1:\n",
    "data_b = rng.binomial(n=1, p=0.067, size=1200)\n",
    "\n",
    "# initialize a test:\n",
    "test = BinaryDataTest()\n",
    "\n",
    "# add variant using raw data (arrays of zeros and ones):\n",
    "test.add_variant_data(\"A\", data_a)\n",
    "test.add_variant_data(\"B\", data_b)\n",
    "# priors can be specified like this (default for this test is a=b=1/2):\n",
    "# test.add_variant_data(\"B\", data_b, a_prior=1, b_prior=20)\n",
    "\n",
    "# add variant using aggregated data (same as raw data with 950 zeros and 50 ones):\n",
    "# test.add_variant_data_agg(\"C\", totals=1000, positives=50)\n",
    "\n",
    "# evaluate test:\n",
    "results = test.evaluate()\n",
    "results\n",
    "print(pd.DataFrame(results).set_index('variant').T.to_markdown(tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99319554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the interpretation of the frequentist 95% confidence interval by dichotomizing it in \n",
    "# statistically significant or non-statistically significant, \n",
    "# hampering a proper discussion on the values, the width (precision) and the practical implications of such interval.\n",
    "#  Interpretation of the Bayesian 95% confidence interval (which is known as credible interval): \n",
    "# there is a 95% probability that the true (unknown) estimate would lie within the interval, given the evidence provided by the observed data.\n",
    "\n",
    "\n",
    "# I have done some research on the Bayesian approach Meta uses and compared it with the Frequentist approach we and Google use.\n",
    "# The high level conclusion is that the confidence score and lower bound reported by Meta is comparable with and equivalent to the confidence level and confidence interval lower bound we report assuming that the true lift follows a normal distribution.\n",
    "# The only subtle difference is the ideology between these two approach which is:\n",
    "# Bayesian: Give observed data, there is a 99% (confidence score) probability that the true value of the lift lies within the credible region (conversions_incremental_lower, conversions_incremental_upper, in the export cell A20 and A21)\n",
    "# Frequentist: If this experiment is repeated many times, 95 out of 100 times the lift will be higher than the 90% confidence interval's lower bound.\n",
    "# This subtlety reflects the fundamental disagreement between the two on the definition of probability.\n",
    "# Bayesianism believes probability is about the degree of certainty about certain events, which is essentially related to the observer's own knowledget about an event.\n",
    "# Frequesntism believes probability is the frequency of certain events.\n",
    "# They only give different results when there are  non Gaussian variables involved (i.e. when a variable doesn't follow normal distribution).\n",
    "\n",
    "# https://trainline.slack.com/archives/C07CK7RMQ95/p1723130821592069\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
